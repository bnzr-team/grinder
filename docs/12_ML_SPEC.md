# GRINDER - ML Specification

> Machine learning for parameter calibration and policy discovery

**Status:** M8 milestone â€” implementation planned
**SSOT:** This document defines ML integration contracts
**See also:** ADR-064 (ML Integration), docs/05_FEATURE_CATALOG.md

---

## Table of Contents

1. [ML Philosophy](#121-ml-philosophy)
2. [M8 I/O Contracts (SSOT)](#122-m8-io-contracts-ssot)
3. [Determinism Invariants](#123-determinism-invariants)
4. [Artifact Versioning Scheme](#124-artifact-versioning-scheme)
5. [Runtime Enablement Model](#125-runtime-enablement-model)
6. [M8 Milestone Plan](#126-m8-milestone-plan)
7. [ML Use Cases](#127-ml-use-cases)
8. [Feature Engineering for ML](#128-feature-engineering-for-ml)
9. [Model Training Pipeline](#129-model-training-pipeline)
10. [Model Registry](#1210-model-registry)
11. [Model Monitoring](#1211-model-monitoring)
12. [Calibration Path](#1212-calibration-path)

---

## 12.1 ML Philosophy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ML IN GRINDER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  ML is NOT for:                                                 â”‚
â”‚  âŒ Predicting price direction                                  â”‚
â”‚  âŒ Timing entries/exits                                        â”‚
â”‚  âŒ Black-box trading decisions                                 â”‚
â”‚                                                                  â”‚
â”‚  ML IS for:                                                     â”‚
â”‚  âœ“ Calibrating grid parameters                                  â”‚
â”‚  âœ“ Optimizing policy thresholds                                 â”‚
â”‚  âœ“ Discovering new policy rules (offline)                       â”‚
â”‚  âœ“ Estimating fill probabilities                                â”‚
â”‚  âœ“ Predicting toxicity regimes                                  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 12.2 M8 I/O Contracts (SSOT)

This section defines the canonical input/output contracts for ML integration.
All implementations MUST conform to these contracts.

### 12.2.1 Input: Feature Snapshots

ML models receive two feature snapshot types as input:

#### FeatureSnapshot (L1 + Volatility)

```python
@dataclass(frozen=True)
class FeatureSnapshot:
    """L1 microstructure + volatility features.

    Source: src/grinder/features/types.py
    """
    ts: int                    # Timestamp (ms)
    symbol: str                # Trading symbol

    # L1 microstructure
    mid_price: Decimal         # Mid price (bid + ask) / 2
    spread_bps: int            # Bid-ask spread in integer bps
    imbalance_l1_bps: int      # L1 imbalance [-10000, 10000]
    thin_l1: Decimal           # Min(bid_qty, ask_qty)

    # Volatility
    natr_bps: int              # Normalized ATR in integer bps
    atr: Decimal | None        # Raw ATR (None if warmup)

    # Range/trend
    sum_abs_returns_bps: int   # Sum of absolute returns
    net_return_bps: int        # Net return over horizon
    range_score: int           # Choppiness indicator

    # Metadata
    warmup_bars: int           # Completed bars available
```

#### L2FeatureSnapshot (Order Book Depth)

```python
@dataclass(frozen=True)
class L2FeatureSnapshot:
    """L2 order book features.

    Source: src/grinder/features/l2_types.py
    Field names: SPEC_V2_0.md Â§B.2 (*_topN_* naming)
    """
    ts_ms: int                 # Timestamp (ms)
    symbol: str                # Trading symbol
    venue: str                 # Exchange venue
    depth: int                 # Levels per side (topN)

    # Depth features
    depth_bid_qty_topN: Decimal
    depth_ask_qty_topN: Decimal
    depth_imbalance_topN_bps: int  # [-10000, 10000]

    # Impact features (VWAP slippage)
    impact_buy_topN_bps: int
    impact_sell_topN_bps: int
    impact_buy_topN_insufficient_depth: int   # 0 or 1
    impact_sell_topN_insufficient_depth: int  # 0 or 1

    # Wall features
    wall_bid_score_topN_x1000: int
    wall_ask_score_topN_x1000: int

    # Config
    qty_ref: Decimal           # Reference qty for impact calc
```

### 12.2.2 Output: MlSignalSnapshot

ML models produce a single output type:

```python
@dataclass(frozen=True)
class MlSignalSnapshot:
    """ML model output signal.

    All probabilities are integers in basis points [0, 10000].
    This ensures deterministic serialization/comparison.

    SSOT: docs/12_ML_SPEC.md Â§12.2.2
    """
    ts: int                    # Timestamp when computed (ms)
    symbol: str                # Trading symbol
    model_version: str         # Model artifact version (e.g., "v1.0.0")
    model_hash: str            # SHA256 of model artifact (16-char hex)

    # Regime probabilities (sum = 10000 bps = 100%)
    regime_low_prob_bps: int   # P(LOW regime) in bps
    regime_mid_prob_bps: int   # P(MID regime) in bps
    regime_high_prob_bps: int  # P(HIGH regime) in bps

    # Predicted regime (argmax of probabilities)
    predicted_regime: str      # "LOW" | "MID" | "HIGH"
    regime_confidence_bps: int # Max probability in bps

    # Parameter adjustments (multipliers as x1000 integers)
    spacing_multiplier_x1000: int   # 1000 = 1.0x, 1500 = 1.5x

    # Feature importance (top 3, for interpretability)
    top_features: tuple[str, str, str]
    top_feature_weights_x1000: tuple[int, int, int]

    # Metadata
    inference_latency_us: int  # Inference time in microseconds
    features_hash: str         # SHA256 of input features (16-char hex)
```

### 12.2.3 Contract Invariants

**Input contracts:**
- All integer fields use basis points (bps) or x1000 encoding
- All Decimal fields serialize as strings
- `ts`/`ts_ms` are Unix milliseconds (int)
- Missing features â†’ model MUST handle gracefully (return neutral signal)

**Output contracts:**
- `regime_*_prob_bps` MUST sum to exactly 10000
- `predicted_regime` MUST be one of: `"LOW"`, `"MID"`, `"HIGH"`
- `regime_confidence_bps` = max(regime_low_prob_bps, regime_mid_prob_bps, regime_high_prob_bps)
- `spacing_multiplier_x1000` MUST be in range [500, 2000] (0.5x to 2.0x)
- `model_hash` MUST match artifact manifest (see Â§12.4)
- `features_hash` = SHA256 of JSON-serialized inputs, truncated to 16 hex chars

**JSON serialization:**
```json
{
  "ts": 1700000000000,
  "symbol": "BTCUSDT",
  "model_version": "v1.0.0",
  "model_hash": "a1b2c3d4e5f67890",
  "regime_low_prob_bps": 2000,
  "regime_mid_prob_bps": 5000,
  "regime_high_prob_bps": 3000,
  "predicted_regime": "MID",
  "regime_confidence_bps": 5000,
  "spacing_multiplier_x1000": 1000,
  "top_features": ["natr_bps", "spread_bps", "depth_imbalance_topN_bps"],
  "top_feature_weights_x1000": [350, 280, 220],
  "inference_latency_us": 1500,
  "features_hash": "f0e1d2c3b4a59687"
}
```

---

## 12.3 Determinism Invariants

ML integration MUST preserve replay determinism. This section lists invariants.

### 12.3.1 MUST (Required)

| Invariant | Description |
|-----------|-------------|
| **DET-01** | Same input features â†’ same MlSignalSnapshot output (bitwise identical) |
| **DET-02** | Model weights loaded from artifact file (no random init) |
| **DET-03** | All floating-point outputs converted to integer bps/x1000 via `round()` |
| **DET-04** | Feature ordering matches canonical order in FeatureSnapshot/L2FeatureSnapshot |
| **DET-05** | Model version + hash included in output for traceability |
| **DET-06** | Inference uses `float32` or `float64` (no mixed precision) |
| **DET-07** | NumPy/ONNX random seed fixed at model load time |
| **DET-08** | features_hash computed BEFORE inference, logged with output |

### 12.3.2 MUST NOT (Forbidden)

| Anti-pattern | Why forbidden |
|--------------|---------------|
| **NODET-01** | Random dropout/noise at inference time |
| **NODET-02** | Time-based or clock-based features in model |
| **NODET-03** | External API calls during inference |
| **NODET-04** | Mutable global state affecting inference |
| **NODET-05** | Float comparison without epsilon tolerance |
| **NODET-06** | Thread-local storage for model state |
| **NODET-07** | Dynamic feature selection based on runtime conditions |

### 12.3.3 Verification Protocol

```bash
# Run determinism check: same inputs â†’ same outputs across 2 runs
python -m scripts.verify_ml_determinism \
    --fixture tests/fixtures/sample_day \
    --model var/models/regime_v1/

# Expected output:
# Input features hash:  a1b2c3d4e5f67890
# Run 1 output hash:    f0e1d2c3b4a59687
# Run 2 output hash:    f0e1d2c3b4a59687
# Determinism check:    âœ… PASS
```

---

## 12.4 Artifact Versioning Scheme

ML model artifacts follow a strict versioning scheme for reproducibility.

### 12.4.1 Directory Structure

```
var/models/
â””â”€â”€ regime_v1/
    â”œâ”€â”€ manifest.json       # Metadata + checksums (SSOT)
    â”œâ”€â”€ model.onnx          # Model weights (ONNX format)
    â”œâ”€â”€ scaler.joblib       # Feature scaler (optional)
    â””â”€â”€ config.json         # Hyperparameters
```

### 12.4.2 manifest.json Schema

```json
{
  "schema_version": 1,
  "model_name": "regime_classifier",
  "model_version": "v1.0.0",
  "created_at": "2024-01-15T10:30:00Z",
  "framework": "lightgbm",
  "export_format": "onnx",

  "files": {
    "model.onnx": {
      "sha256": "a1b2c3d4e5f6789012345678901234567890123456789012345678901234abcd",
      "size_bytes": 102400
    },
    "scaler.joblib": {
      "sha256": "b2c3d4e5f6789012345678901234567890123456789012345678901234abcde",
      "size_bytes": 2048
    },
    "config.json": {
      "sha256": "c3d4e5f6789012345678901234567890123456789012345678901234abcdef",
      "size_bytes": 512
    }
  },

  "model_hash": "a1b2c3d4e5f67890",

  "input_features": [
    "natr_bps",
    "spread_bps",
    "imbalance_l1_bps",
    "range_score",
    "depth_imbalance_topN_bps",
    "impact_buy_topN_bps",
    "impact_sell_topN_bps"
  ],

  "output_classes": ["LOW", "MID", "HIGH"],

  "training_metadata": {
    "train_samples": 50000,
    "val_samples": 10000,
    "test_accuracy_bps": 7250,
    "data_hash": "d4e5f67890123456"
  }
}
```

### 12.4.3 Checksum Rules

| Field | Computation |
|-------|-------------|
| `files[*].sha256` | SHA256 of file contents (64-char hex) |
| `model_hash` | SHA256 of `model.onnx` truncated to 16 hex chars |
| `data_hash` | SHA256 of training data, truncated to 16 hex chars |

### 12.4.4 Version Naming

```
<model_name>_v<major>.<minor>.<patch>

Examples:
- regime_classifier_v1.0.0  # Initial release
- regime_classifier_v1.0.1  # Bugfix (same architecture)
- regime_classifier_v1.1.0  # New features (backward compatible)
- regime_classifier_v2.0.0  # Breaking change (new I/O contract)
```

**Semantic versioning rules:**
- MAJOR: Breaking changes to I/O contract
- MINOR: New features, backward compatible
- PATCH: Bugfixes, retraining with same architecture

### 12.4.5 Artifact Validation

```python
def validate_artifact(artifact_dir: Path) -> bool:
    """Validate model artifact integrity.

    Returns True if all checksums match manifest.
    """
    manifest = json.load((artifact_dir / "manifest.json").open())

    for filename, meta in manifest["files"].items():
        file_path = artifact_dir / filename
        actual_hash = hashlib.sha256(file_path.read_bytes()).hexdigest()
        if actual_hash != meta["sha256"]:
            raise ArtifactCorruptedError(f"{filename}: hash mismatch")

    return True
```

---

## 12.5 Runtime Enablement Model

ML integration is **disabled by default** for safe rollout.

### 12.5.1 Configuration

```yaml
# config/paper.yaml or config/live.yaml
ml:
  enabled: false              # MUST be explicitly enabled
  model_dir: "var/models/regime_v1"

  # Safety limits
  max_inference_latency_ms: 10
  fallback_on_error: "neutral"  # "neutral" | "previous" | "error"

  # Feature toggles
  use_l2_features: true
  use_l1_features: true
```

### 12.5.2 Enablement Hierarchy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ML ENABLEMENT HIERARCHY                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Level 1: Global gate                                          â”‚
â”‚  â””â”€â”€ ml.enabled = false (default)                              â”‚
â”‚      â””â”€â”€ If false: skip ML entirely, use rule-based policy     â”‚
â”‚                                                                 â”‚
â”‚  Level 2: Artifact gate                                        â”‚
â”‚  â””â”€â”€ Model artifact must exist and validate                    â”‚
â”‚      â””â”€â”€ If missing/invalid: log warning, use rule-based       â”‚
â”‚                                                                 â”‚
â”‚  Level 3: Runtime gate                                         â”‚
â”‚  â””â”€â”€ Inference must complete within max_latency_ms             â”‚
â”‚      â””â”€â”€ If timeout: log warning, use fallback                 â”‚
â”‚                                                                 â”‚
â”‚  Level 4: Output gate                                          â”‚
â”‚  â””â”€â”€ MlSignalSnapshot must pass contract validation            â”‚
â”‚      â””â”€â”€ If invalid: log error, use fallback                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 12.5.3 Fallback Behavior

| `fallback_on_error` | Behavior |
|---------------------|----------|
| `"neutral"` | Return neutral signal: `predicted_regime="MID"`, `spacing_multiplier_x1000=1000` |
| `"previous"` | Return last valid signal for this symbol |
| `"error"` | Raise exception, halt processing |

### 12.5.4 Safe Rollout Path

```
Phase 1: Shadow mode (ml.enabled=true, ml.shadow_only=true)
â”œâ”€â”€ ML runs in parallel with rule-based
â”œâ”€â”€ ML output logged but NOT used for decisions
â””â”€â”€ Compare ML vs rule-based performance

Phase 2: Canary (ml.enabled=true, ml.canary_pct=5)
â”œâ”€â”€ 5% of symbols use ML decisions
â”œâ”€â”€ Monitor for degradation
â””â”€â”€ Auto-rollback if metrics degrade

Phase 3: Full rollout (ml.enabled=true)
â”œâ”€â”€ All symbols use ML decisions
â”œâ”€â”€ Continuous monitoring
â””â”€â”€ Manual rollback available
```

---

## 12.6 M8 Milestone Plan

M8 (ML Integration) is divided into three sub-milestones.

### M8-00: Specification (docs-only)

**Scope:** This document
**Deliverables:**
- [x] I/O contracts (FeatureSnapshot â†’ MlSignalSnapshot)
- [x] Determinism invariants (MUST/MUST NOT)
- [x] Artifact versioning scheme (manifest.json + sha256)
- [x] Runtime enablement model (ml_enabled=False default)
- [x] M8 milestone plan

**Acceptance criteria:**
- All sections present in docs/12_ML_SPEC.md
- ADR-064 created in docs/DECISIONS.md
- No code changes

### M8-01: Stub Implementation âœ…

**Scope:** Code scaffold with tests, no real model
**Deliverables:**
- [x] `MlSignalSnapshot` dataclass in `src/grinder/ml/__init__.py` (PR #140)
- [x] Time-indexed signal selection `_get_ml_signal(symbol, ts_ms)` with bisect O(log n) (PR #141)
- [x] Integration point in PaperEngine (`ml_enabled` flag)
- [x] 26 unit tests for contracts (14 contract + 12 selection)
- [x] Digest-locked fixtures: `sample_day_ml_multisignal_basic`, `sample_day_ml_multisignal_no_prior` (PR #142)

**Acceptance criteria:**
- [x] `ml_enabled=False` â†’ no ML code path executed
- [x] `ml_enabled=True` + no signal.json â†’ baseline digest (safe-by-default)
- [x] SSOT selection rule: max(signal.ts_ms) where signal.ts_ms <= snapshot.ts_ms
- [x] All tests pass, digest unchanged when disabled

### M8-02: ONNX Integration

**Scope:** Real model inference via ONNX Runtime

#### M8-02a: Artifact Plumbing (no inference) âœ…

**Deliverables:**
- [x] `OnnxArtifactManifest` and `OnnxArtifact` types in `src/grinder/ml/onnx/`
- [x] Artifact loader with SHA256 validation
- [x] Config fields: `ml_shadow_mode`, `ml_infer_enabled`, `onnx_artifact_dir`
- [x] `verify_onnx_artifact.py` script
- [x] 19 unit tests for artifact validation

**ONNX Artifact v1 Format (legacy):**
```json
{
  "schema_version": "v1",
  "model_file": "model.onnx",
  "sha256": {
    "model.onnx": "a1b2c3..."
  },
  "created_at": "2026-02-14T00:00:00Z",
  "notes": "optional"
}
```

**ONNX Artifact v1.1 Format (M8-03a):**
```json
{
  "schema_version": "v1.1",
  "model_file": "model.onnx",
  "sha256": {
    "model.onnx": "a1b2c3d4e5f67890..."
  },
  "created_at": "2026-02-14T12:00:00Z",
  "created_at_utc": "2026-02-14T12:00:00Z",
  "git_sha": "abc123def456...",
  "dataset_id": "train_2026Q1",
  "feature_order": ["price_mid", "price_bid", ...],
  "notes": "optional"
}
```

**v1.1 Additional Fields:**
- `created_at_utc`: UTC ISO8601 timestamp (explicit timezone)
- `git_sha`: 40-char hex git commit SHA (null if unavailable)
- `dataset_id`: Training dataset identifier (required for reproducibility)
- `feature_order`: List of feature names in expected order (SSOT validation)

**Validation rules:**
- `schema_version` must be `"v1"` or `"v1.1"`
- `model_file` must exist in `sha256` map
- All paths must be relative (no `..`, no absolute)
- SHA256 must match actual file content
- `git_sha` must be 40-char hex string (if present)
- `feature_order` is validated against SSOT `FEATURE_ORDER` (warning on mismatch)

**Safe-by-default:**
- `ml_shadow_mode=False` (default)
- `ml_infer_enabled=False` (default)
- `onnx_artifact_dir=None` (default)
- Existing digests unchanged when all flags are off

#### M8-02b: Shadow Mode âœ…

**Deliverables:**
- [x] `OnnxMlModel` with `load_from_dir()` and `predict()` methods
- [x] `vectorize()` for feature vectorization with `FEATURE_ORDER` SSOT
- [x] Model loading with soft-fail (returns None on error)
- [x] Shadow logging (predicted regime, latency metrics)
- [x] Config validation guards (ort check, shadow+infer combo, artifact_dir)
- [x] 19 unit tests (9 model tests + 10 shadow mode tests)
- [x] Tiny test ONNX artifact (`tiny_regime/`)

**ONNX Runtime Integration:**
- `onnxruntime>=1.17,<2.0` added to `[ml]` extras
- `ONNX_AVAILABLE` constant for optional import
- CPU-only execution for determinism (single-threaded)

**Feature Vectorization:**
```python
FEATURE_ORDER = (
    "price_mid", "price_bid", "price_ask", "spread_bps",
    "volume_24h", "volume_1h",
    "volatility_1h_bps", "volatility_24h_bps",
    "position_size", "position_notional", "position_pnl_bps",
    "grid_levels_active", "grid_utilization_pct",
    "trend_strength", "momentum_1h",
)
```

**Config Guards:**
1. `ml_infer_enabled=True && !ONNX_AVAILABLE` â†’ ConfigError
2. `ml_infer_enabled=True && !ml_shadow_mode` â†’ ConfigError (real inference not yet supported)
3. `ml_shadow_mode=True && !ml_infer_enabled` â†’ ConfigError
4. `ml_shadow_mode=True && !onnx_artifact_dir` â†’ ConfigError

#### M8-02c: Active Inference Mode

**Reference:** ADR-065 (Shadow â†’ Active Inference Transition)

**Sub-PRs:**
- M8-02c-0: ADR-065 (docs only) â€” defines state machine, guards, observability
- M8-02c-1: Config guards + tests (no inference logic)
- M8-02c-2: Active inference implementation

**Key Safety Requirements (from ADR-065):**
- Two-key activation: `ml_infer_enabled` + `ml_active_enabled`
- Explicit ack: `ml_active_ack="I_UNDERSTAND_THIS_AFFECTS_TRADING"`
- Kill-switch: `ML_KILL_SWITCH=1` env var (per-snapshot, instant OFF)
- Fail-closed: ACTIVE errors do NOT mutate policy

**Acceptance criteria:**
- Determinism verified across 2 runs
- Inference latency < 10ms on sample fixture
- 15 guard tests per ADR-065 Test Requirements
- Kill-switch activates immediately (per-snapshot)

#### M8-02c-3: Structured Logs & Observability

**Status:** âœ… Implemented (PR #149)

**Log Events (SSOT):**

| Event | Level | When | Key Fields |
|-------|-------|------|------------|
| `ML_MODE_INIT` | info | Model loaded | `mode`, `artifact_dir` |
| `ML_KILL_SWITCH_ON` | warning | Kill-switch active | `ts`, `symbol`, `reason` |
| `ML_ACTIVE_ON` | info | Inference succeeded | `ts`, `symbol` |
| `ML_ACTIVE_BLOCKED` | warning | ACTIVE blocked | `ts`, `symbol`, `reason`, `latency_ms` (optional) |
| `ML_INFER_OK` | info | Inference success | `ts`, `symbol`, `regime`, `probs_bps`, `spacing_x1000`, `latency_ms`, `artifact_dir` |
| `ML_INFER_ERROR` | error | Inference exception | `ts`, `symbol`, `error`, `latency_ms`, `artifact_dir` |

**Prometheus Metrics:**

| Metric | Type | Description |
|--------|------|-------------|
| `grinder_ml_active_on` | gauge | 1 if ACTIVE inference succeeded this tick, 0 otherwise |
| `grinder_ml_block_total{reason="..."}` | counter | Count of ACTIVE blocks by reason code |
| `grinder_ml_inference_total` | counter | Total successful inferences |
| `grinder_ml_inference_errors_total` | counter | Total inference errors |

**Reason Codes (MlBlockReason enum, priority order):**

1. `KILL_SWITCH_ENV` - `ML_KILL_SWITCH=1` env var active
2. `KILL_SWITCH_CONFIG` - `ml_kill_switch=True` config
3. `INFER_DISABLED` - `ml_infer_enabled=False`
4. `ACTIVE_DISABLED` - `ml_active_enabled=False`
5. `BAD_ACK` - `ml_active_ack` != expected string
6. `ONNX_UNAVAILABLE` - `onnxruntime` not installed
7. `ARTIFACT_DIR_MISSING` - `onnx_artifact_dir` not found
8. `MANIFEST_INVALID` - `manifest.json` invalid or missing
9. `MODEL_NOT_LOADED` - ONNX model is `None`
10. `ENV_NOT_ALLOWED` - `GRINDER_ENV` not in allowlist

**Source files:**
- `src/grinder/ml/metrics.py` - SSOT for reason codes, gauge state, Prometheus export
- `src/grinder/paper/engine.py` - Log emission points
- `tests/unit/test_ml_log_events.py` - caplog tests for log events

### M8-03: Training & Export Pipeline

**Scope:** Reproducible training pipeline for ONNX model artifacts.

#### M8-03a: Artifact Pack Spec + Build CLI

**Status:** ğŸš§ In Progress

**Scope:** Extend manifest to v1.1, add build CLI for artifact generation

**Deliverables:**
- [x] Extended `OnnxArtifactManifest` with v1.1 fields (`git_sha`, `dataset_id`, `feature_order`)
- [x] `build_onnx_artifact.py` CLI script
- [x] `validate_feature_order()` function with SSOT validation (warning on mismatch)
- [x] Unit tests for v1.1 manifest and build CLI

**CLI Usage:**
```bash
python -m scripts.build_onnx_artifact \
    --model-path trained_model.onnx \
    --output-dir artifacts/regime_v1 \
    --dataset-id train_2026Q1 \
    --notes "Optional notes"
```

**Build process:**
1. Copy model file to output directory
2. Compute SHA256 checksum
3. Auto-detect git SHA (graceful fallback to null + warning)
4. Generate v1.1 manifest with `FEATURE_ORDER` from SSOT
5. Write manifest.json

**Acceptance criteria:**
- Build CLI creates valid v1.1 artifact
- Artifact passes `verify_onnx_artifact.py` validation
- `feature_order` matches SSOT (warning logged if mismatch)
- `git_sha` null with warning if not in git repo

#### M8-03b-1: Training/Export Pipeline MVP

**Status:** âœ… Done (PR #152)

**Deliverables:**
- [x] `scripts/train_regime_model.py` CLI for training and ONNX export
- [x] Deterministic data generation with seed control
- [x] RandomForest classifier â†’ ONNX conversion via skl2onnx
- [x] Golden test artifact (`tests/testdata/onnx_artifacts/golden_regime/`)
- [x] 23 unit tests for training pipeline
- [x] 5 integration tests for trainâ†’artifactâ†’inference roundtrip

**CLI Usage:**
```bash
# Basic training with defaults
python -m scripts.train_regime_model \
    --out-dir /tmp/regime_v1 \
    --dataset-id production_data

# Custom parameters
python -m scripts.train_regime_model \
    --out-dir /tmp/regime_v2 \
    --dataset-id production_data \
    --seed 42 \
    --n-samples 1000 \
    --notes "Initial production model"
```

**Output Artifact:**
```
<out-dir>/
â”œâ”€â”€ model.onnx          # ONNX model file
â”œâ”€â”€ manifest.json       # Artifact manifest with SHA256
â””â”€â”€ train_report.json   # Training metrics and metadata
```

**train_report.json Schema:**
```json
{
  "dataset_id": "production_data",
  "n_samples": 1000,
  "seed": 42,
  "n_features": 15,
  "train_accuracy": 0.95,
  "regime_distribution": {"LOW": 50, "MID": 400, "HIGH": 550},
  "created_at": "2026-02-15T00:00:00Z",
  "model_sha256": "abc123...",
  "onnx_opset_version": 15,
  "sklearn_version": "1.8.0",
  "skl2onnx_version": "1.20.0"
}
```

**Determinism Guarantees:**
- Same `--seed` + `--dataset-id` + `--n-samples` â†’ identical model SHA256
- Achieved by:
  1. Deterministic synthetic data generation (numpy rng with combined seed)
  2. sklearn RandomForest with `random_state=seed`, `n_jobs=1`
  3. Fixed ONNX graph name (skl2onnx defaults to random UUID)

**Model Contract:**
- Input: `"input"` tensor shape `(batch, 15)` matching `FEATURE_ORDER`
- Output: `"regime_probs"` tensor shape `(batch, 3)` for [LOW, MID, HIGH]
- No spacing_multiplier in MVP (defaults to 1.0 in OnnxMlModel)

**Dependencies:**
```toml
# pyproject.toml [project.optional-dependencies]
ml = [
    "scikit-learn>=1.4,<2.0",
    "onnx>=1.15,<2.0",
    "onnxruntime>=1.17,<2.0",
    "skl2onnx>=1.16,<2.0",  # M8-03b: sklearn to ONNX conversion
]
```

**Source files:**
- `scripts/train_regime_model.py` - Training CLI
- `tests/unit/test_train_regime_model.py` - Unit tests
- `tests/integration/test_train_to_artifact_roundtrip.py` - Integration tests
- `tests/testdata/onnx_artifacts/golden_regime/` - Golden test artifact

#### M8-03b-2: Runtime Integration & Determinism

**Status:** âœ… Done (PR #153)

**Scope:** Validate that artifacts from M8-03b-1 integrate correctly with `OnnxMlModel`
runtime and produce bit-for-bit identical predictions for fixed inputs.

**Deliverables:**
- [x] Golden artifact runtime tests (load twice â†’ predict â†’ compare)
- [x] `test_vectorize_order_matches_feature_order_exactly` - SSOT contract verification
- [x] Full FEATURE_ORDER fixture (all 15 features populated)
- [x] Multiple prediction stability test

**Determinism Guarantees:**

Two types of determinism are validated:

1. **Training determinism** (M8-03b-1):
   - Same `--seed` + `--dataset-id` + `--n-samples` â†’ identical model SHA256
   - Verified by comparing model hashes from two independent training runs

2. **Runtime determinism** (M8-03b-2):
   - Same model file + same input features â†’ identical output
   - Verified by loading model twice, predicting, and comparing `regime_probs_bps`
   - No float comparison (all outputs are quantized to bps/x1000 integers)

**Test Matrix:**

| Test | What it validates |
|------|-------------------|
| `test_golden_load_twice_predict_identical` | Runtime determinism across model instances |
| `test_golden_multiple_predictions_stable` | Prediction stability within single instance |
| `test_golden_predict_with_full_feature_vector` | All 15 FEATURE_ORDER features work |
| `test_vectorize_order_matches_feature_order_exactly` | SSOT vectorize contract |
| `test_vectorize_preserves_feature_order_tuple` | FEATURE_ORDER is immutable (15 elements) |

**Source files:**
- `tests/unit/test_onnx_model.py` - Runtime determinism tests
- `src/grinder/ml/onnx/features.py` - SSOT FEATURE_ORDER and vectorize()

### M8-03c: Model Registry & Promotion

**Scope:** Governance layer for model lifecycle: versioning, promotion, rollback, audit trail.

#### M8-03c-1a: Registry Spec & Runbook

**Status:** âœ… Done (PR #155)

**Goal:** Define SSOT model registry schema and promotion policy.

#### Registry Schema (v1)

Registry file: `ml/registry/models.json`

```json
{
  "schema_version": "v1",
  "models": {
    "regime_classifier": {
      "active": {
        "artifact_id": "regime_v2026_02_15_001",
        "artifact_dir": "artifacts/onnx/regime_v2026_02_15_001",
        "model_sha256": "...",
        "git_sha": "abc123",
        "dataset_id": "production_data_v3",
        "created_at_utc": "2026-02-15T10:00:00Z",
        "promoted_at_utc": "2026-02-15T12:00:00Z",
        "notes": "Promoted after 24h shadow pass"
      },
      "shadow": {
        "artifact_id": "regime_v2026_02_16_001",
        "artifact_dir": "artifacts/onnx/regime_v2026_02_16_001",
        "model_sha256": "...",
        "git_sha": "def456",
        "dataset_id": "production_data_v4",
        "created_at_utc": "2026-02-16T08:00:00Z",
        "notes": "Shadow testing new dataset"
      },
      "staging": null,
      "history": [
        {
          "timestamp_utc": "2026-02-15T12:00:00Z",
          "action": "promote",
          "from_stage": "shadow",
          "to_stage": "active",
          "artifact_id": "regime_v2026_02_15_001",
          "by": "operator@example.com",
          "reason": "SLO compliance: p99<50ms, error rate<1%"
        }
      ]
    }
  }
}
```

#### Environments (Stages)

| Stage | Purpose | Policy Impact | Rollback |
|-------|---------|---------------|----------|
| **staging** | Pre-flight validation | None | Delete entry |
| **shadow** | Shadow mode inference | Metrics only | Swap artifact_id |
| **active** | Live trading decisions | Affects orders | Swap artifact_id (PR) |

#### Promotion Flow

```
staging â†’ shadow â†’ active
                â†‘
                â””â”€â”€ rollback (swap artifact_id to previous)
```

#### Promotion Requirements

**staging â†’ shadow:**
- [ ] `verify_onnx_artifact` PASS
- [ ] Unit tests pass on artifact
- [ ] Golden test determinism PASS

**shadow â†’ active:**
- [ ] 24h shadow window (configurable)
- [ ] Latency SLO: p99 < 100ms, p99.9 < 250ms
- [ ] Error rate < 5%
- [ ] No MlActiveModePersistentlyBlocked alerts (unless intentional)
- [ ] Dashboard review: baseline stable
- [ ] Explicit ACK: `--ack I_UNDERSTAND_THIS_AFFECTS_TRADING`

**active â†’ rollback:**
- [ ] PR changing `active.artifact_id` to previous
- [ ] No ACK required (emergency)
- [ ] Post-rollback: investigate root cause

#### Artifact ID Convention

Format: `{model_name}_v{YYYY}_{MM}_{DD}_{seq}`

Examples:
- `regime_v2026_02_15_001` â€” first regime model on Feb 15
- `regime_v2026_02_15_002` â€” second regime model same day
- `spacing_v2026_02_16_001` â€” spacing model

#### Config Integration

```yaml
# grinder config
ml_registry_path: "ml/registry/models.json"  # SSOT
ml_model_name: "regime_classifier"           # Model to load
ml_stage: "shadow"                           # staging|shadow|active

# Legacy fallback (if ml_registry_path not set)
onnx_artifact_dir: "artifacts/onnx/regime_v2026_02_15_001"
```

**Resolution order:**
1. If `ml_registry_path` set â†’ load artifact_dir from registry by `ml_model_name` + `ml_stage`
2. Else â†’ use `onnx_artifact_dir` directly (backward compatible)

#### Source files (planned)
- `ml/registry/models.json` - SSOT registry
- `src/grinder/ml/onnx/registry.py` - Registry loader
- `scripts/verify_ml_registry.py` - Validation CLI
- `scripts/promote_model.py` - Promotion CLI (optional)
- `docs/runbooks/19_ML_MODEL_PROMOTION.md` - Operational runbook

#### M8-03c-1b: Registry Implementation

**Status:** ğŸš§ In Review (PR #157)

**Goal:** Implement Git-based model registry with strict validation and CLI tools.

**Deliverables:**
- [x] `ml/registry/models.json` - SSOT registry file pointing to model artifacts
- [x] `src/grinder/ml/onnx/registry.py` - Registry loader with strict validation
- [x] `scripts/verify_ml_registry.py` - CLI tool to validate registry and artifacts
- [x] 18 unit tests for registry validation
- [x] 3 integration tests for registry â†’ artifact â†’ predict flow
- [x] `ml/registry/README.md` - Registry usage documentation

**Registry Loader API:**

```python
from grinder.ml.onnx.registry import ModelRegistry, Stage

# Load registry from Git SSOT
registry = ModelRegistry.load("ml/registry/models.json")

# Get artifact directory for specific stage
pointer = registry.get_stage_pointer("regime_classifier", Stage.SHADOW)
if pointer:
    artifact_dir = registry.resolve_artifact_dir(pointer, base_dir=Path("."))
    # Load artifact using existing load_artifact()
    artifact = load_artifact(artifact_dir)
```

**Validation Rules:**

| Rule | Enforcement |
|------|-------------|
| No path traversal (`..`) | Hard error |
| No absolute paths | Hard error |
| `git_sha` must be 40-char hex (if not null) | Hard error |
| `artifact_dir` must exist | Error (deferred to runtime) |
| Model name must match `[a-z0-9_]+` | Hard error |
| Stage must be `shadow`, `active`, or `staging` | Hard error |
| Pointer can be `null` (stage not configured) | Allowed |

**CLI Usage:**

```bash
# Validate registry schema and all referenced artifacts
python -m scripts.verify_ml_registry \
    --path ml/registry/models.json \
    --base-dir .

# Expected output:
# Loading registry: ml/registry/models.json
# Registry schema: v1
# Models: 1
#
# === Model: regime_classifier ===
#   SHADOW:
#     artifact_id: golden_regime_v1
#     artifact_dir: tests/testdata/onnx_artifacts/golden_regime
#     resolved_path: /home/user/grinder/tests/testdata/onnx_artifacts/golden_regime
#     âœ“ Artifact valid: 2 files verified
#
# âœ“ All checks passed
```

**Security Design:**

- **Fail-closed**: Invalid registry â†’ error, no fallback
- **Path safety**: All paths validated before resolution
- **Git-based SSOT**: Registry committed to Git, reviewed in PR
- **No symbolic link traversal**: `Path.resolve()` used for containment check

**Test Coverage:**

Unit tests ([tests/unit/test_ml_registry.py](../../tests/unit/test_ml_registry.py:1)):
- Schema validation (v1 only)
- Path traversal blocking (`../../etc/passwd`)
- Absolute path blocking (`/tmp/model`)
- git_sha validation (40-char hex, null allowed)
- Model name pattern validation
- Stage enum validation

Integration tests ([tests/integration/test_registry_to_predict_roundtrip.py](../../tests/integration/test_registry_to_predict_roundtrip.py:1)):
- Load registry â†’ resolve artifact â†’ load artifact â†’ predict
- Verify predictions match expected structure (regime_probs_bps dict)
- Validate MlSignalSnapshot contract (sum = 10000 bps)

**Source files:**
- [ml/registry/models.json](../../ml/registry/models.json:1) - SSOT registry
- [src/grinder/ml/onnx/registry.py](../../src/grinder/ml/onnx/registry.py:1) - Registry loader (268 lines)
- [scripts/verify_ml_registry.py](../../scripts/verify_ml_registry.py:1) - Validation CLI (167 lines)
- [tests/unit/test_ml_registry.py](../../tests/unit/test_ml_registry.py:1) - 18 unit tests
- [tests/integration/test_registry_to_predict_roundtrip.py](../../tests/integration/test_registry_to_predict_roundtrip.py:1) - 3 integration tests
- [ml/registry/README.md](../../ml/registry/README.md:1) - Usage guide

---

#### M8-03c-2: PaperEngine Config Wiring

**Status:** ğŸš§ In Progress (PR #TBD)

**Goal:** Wire ML registry into PaperEngine runtime for artifact resolution.

**Deliverables:**
- [x] Add `ml_registry_path`, `ml_model_name`, `ml_stage` config fields to PaperEngine
- [x] Implement `_resolve_onnx_artifact_dir()` with resolution order: registry â†’ legacy â†’ none
- [x] Fail-closed guard for ACTIVE mode (no legacy fallback)
- [x] Update determinism suite to accept new registry config fields
- [x] Unit tests for registry resolution paths
- [ ] Integration test: determinism suite â†’ registry â†’ artifact â†’ predict

**Resolution Logic:**

```python
def _resolve_onnx_artifact_dir(self) -> tuple[str | None, str]:
    """Resolve ONNX artifact directory (M8-03c-2).

    Resolution order:
    1. Registry (ml_registry_path + ml_model_name + ml_stage)
    2. Legacy (onnx_artifact_dir) - SHADOW only
    3. None (ML disabled)

    Returns:
        (artifact_dir, source) where source in {"registry", "legacy", "none"}
    """
    # Path 1: Registry resolution
    if self._ml_registry_path and self._ml_model_name:
        registry = ModelRegistry.load(Path(self._ml_registry_path))
        pointer = registry.get_stage_pointer(self._ml_model_name, self._ml_stage)
        if pointer is None:
            raise ValueError(f"No {self._ml_stage} pointer for {self._ml_model_name}")
        artifact_dir = registry.resolve_artifact_dir(pointer, base_dir)
        return str(artifact_dir), "registry"

    # Path 2: Legacy fallback (onnx_artifact_dir)
    if self._onnx_artifact_dir:
        return self._onnx_artifact_dir, "legacy"

    # Path 3: No artifact configured
    return None, "none"
```

**Fail-Closed Guards:**

| Guard | Rule | Enforcement |
|-------|------|-------------|
| G-REG-1 | ACTIVE mode + legacy source â†’ error | Hard fail at init |
| G-REG-2 | ACTIVE mode + registry missing â†’ error | Hard fail at init |
| G-REG-3 | Registry resolution failure â†’ error | Hard fail at init |
| G-REG-4 | SHADOW mode + legacy â†’ allowed | Backward compat |

**PaperEngine Config:**

```python
# New registry-based config (preferred)
engine = PaperEngine(
    ml_shadow_mode=True,
    ml_infer_enabled=True,
    ml_registry_path="ml/registry/models.json",
    ml_model_name="regime_classifier",
    ml_stage="shadow",  # default: "shadow"
)

# Legacy config (backward compatible, SHADOW only)
engine = PaperEngine(
    ml_shadow_mode=True,
    ml_infer_enabled=True,
    onnx_artifact_dir="tests/testdata/onnx_artifacts/golden_regime",
)

# ACTIVE mode (must use registry)
engine = PaperEngine(
    ml_active_enabled=True,
    ml_infer_enabled=True,
    ml_active_ack="I_UNDERSTAND_THIS_AFFECTS_TRADING",
    ml_registry_path="ml/registry/models.json",
    ml_model_name="regime_classifier",
    ml_stage="active",
)
```

**Determinism Suite Integration:**

```yaml
# tests/determinism/golden_suite/paper_with_ml_registry/fixture_config.json
{
  "ml_enabled": false,
  "ml_shadow_mode": true,
  "ml_infer_enabled": true,
  "ml_registry_path": "ml/registry/models.json",
  "ml_model_name": "regime_classifier",
  "ml_stage": "shadow"
}
```

**Test Coverage:**

Unit tests ([tests/unit/test_paper_engine_ml_registry.py](../../tests/unit/test_paper_engine_ml_registry.py:1)):
- Registry resolution in SHADOW mode
- Registry resolution in ACTIVE mode
- Legacy fallback in SHADOW mode
- ACTIVE mode blocks legacy (G-REG-1)
- Registry resolution failure handling
- Stage pointer null handling
- Default stage="shadow"

**Source files:**
- [src/grinder/paper/engine.py](../../src/grinder/paper/engine.py:563) - Resolution logic (56 lines)
- [scripts/verify_determinism_suite.py](../../scripts/verify_determinism_suite.py:84) - Config wiring (79 lines)
- [tests/unit/test_paper_engine_ml_registry.py](../../tests/unit/test_paper_engine_ml_registry.py:1) - 9 unit tests

**Next Steps:**
- M8-03c-3: Promotion CLI script (`promote_model.py`)

---

## 12.7 ML Use Cases

### Use Case 1: Parameter Calibration

**Goal**: Find optimal grid parameters (spacing, levels, sizes) for current market regime.

```python
@dataclass
class CalibrationTarget:
    """What we're optimizing for."""
    metric: str  # "sharpe", "rt_expectancy", "fill_rate"
    constraints: dict[str, tuple[float, float]]  # {"max_dd": (0, 0.05)}

class ParameterCalibrator:
    """Calibrate grid parameters using ML."""

    def __init__(self, model: BaseEstimator):
        self.model = model
        self.feature_names: list[str] = []

    def fit(self, historical_data: pd.DataFrame) -> None:
        """
        Fit calibration model.

        Features: market regime features
        Target: optimal parameters from walk-forward
        """
        X = historical_data[self.feature_names]
        y = historical_data["optimal_spacing"]  # From walk-forward

        self.model.fit(X, y)

    def predict_parameters(self, features: dict) -> dict[str, float]:
        """Predict optimal parameters for current conditions."""
        X = pd.DataFrame([features])[self.feature_names]
        predictions = self.model.predict(X)

        return {
            "spacing_bps": float(predictions[0]),
            # ... other parameters
        }
```

### Use Case 2: Toxicity Prediction

**Goal**: Predict toxicity regime transitions before they happen.

```python
class ToxicityPredictor:
    """Predict toxicity regime changes."""

    def __init__(self):
        self.model = LGBMClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
        )
        self.classes = ["LOW", "MID", "HIGH"]

    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:
        """Train on historical toxicity transitions."""
        self.model.fit(X, y)

    def predict_proba(self, features: dict) -> dict[str, float]:
        """Predict probability of each toxicity regime."""
        X = pd.DataFrame([features])
        probs = self.model.predict_proba(X)[0]
        return dict(zip(self.classes, probs))

    def predict_transition(self, current_regime: str,
                           features: dict) -> tuple[str, float]:
        """Predict most likely next regime and confidence."""
        probs = self.predict_proba(features)
        next_regime = max(probs, key=probs.get)
        confidence = probs[next_regime]

        return next_regime, confidence
```

### Use Case 3: Fill Probability Model

**Goal**: Estimate probability of limit order filling within time horizon.

```python
class FillProbabilityModel:
    """ML model for fill probability estimation."""

    def __init__(self):
        self.model = LGBMRegressor(
            n_estimators=50,
            max_depth=4,
        )

    def fit(self, orders: pd.DataFrame) -> None:
        """
        Train on historical order outcomes.

        Features:
        - distance_from_mid_bps
        - spread_bps
        - ofi_zscore
        - depth_imbalance
        - volatility

        Target: did_fill (0/1) or time_to_fill
        """
        features = [
            "distance_bps", "spread_bps", "ofi_zscore",
            "depth_imbalance", "natr_14_5m"
        ]
        X = orders[features]
        y = orders["filled"]

        self.model.fit(X, y)

    def predict(self, order_price: float, mid: float,
                features: dict) -> float:
        """Predict fill probability."""
        distance_bps = abs(order_price - mid) / mid * 10000

        X = pd.DataFrame([{
            "distance_bps": distance_bps,
            "spread_bps": features["spread_bps"],
            "ofi_zscore": features.get("ofi_zscore", 0),
            "depth_imbalance": features.get("depth_imbalance", 0),
            "natr_14_5m": features["natr_14_5m"],
        }])

        return float(np.clip(self.model.predict(X)[0], 0, 1))
```

### Use Case 4: Policy Discovery (Offline)

**Goal**: Discover new policy rules from historical data.

```python
class PolicyDiscovery:
    """Discover optimal policy rules from data."""

    def __init__(self):
        self.tree = DecisionTreeClassifier(
            max_depth=5,
            min_samples_leaf=100,
        )

    def discover_rules(self, data: pd.DataFrame) -> list[PolicyRule]:
        """
        Discover decision rules for policy selection.

        Features: market conditions
        Target: best performing policy
        """
        X = data[self.feature_names]
        y = data["best_policy"]

        self.tree.fit(X, y)

        # Extract rules
        rules = self._extract_rules(self.tree)

        # Validate rules
        validated_rules = [
            rule for rule in rules
            if self._validate_rule(rule, data)
        ]

        return validated_rules

    def _extract_rules(self, tree: DecisionTreeClassifier) -> list[PolicyRule]:
        """Extract human-readable rules from decision tree."""
        rules = []
        tree_rules = export_text(tree, feature_names=self.feature_names)

        # Parse tree rules into PolicyRule objects
        # ...

        return rules
```

---

## 12.8 Feature Engineering for ML

```python
class MLFeatureEngine:
    """Generate features for ML models."""

    def __init__(self):
        self.scalers: dict[str, StandardScaler] = {}

    def compute_features(self, raw_features: dict,
                         lookback_features: list[dict]) -> dict:
        """
        Compute ML features from raw features.

        Includes:
        - Current values
        - Rolling statistics
        - Rate of change
        - Cross-feature interactions
        """
        features = {}

        # Current values (normalized)
        for key, value in raw_features.items():
            features[f"{key}_current"] = self._normalize(key, value)

        # Rolling statistics
        if len(lookback_features) >= 10:
            for key in raw_features.keys():
                values = [f.get(key, 0) for f in lookback_features[-10:]]
                features[f"{key}_mean_10"] = np.mean(values)
                features[f"{key}_std_10"] = np.std(values)
                features[f"{key}_trend"] = self._calc_trend(values)

        # Rate of change
        if len(lookback_features) >= 2:
            prev = lookback_features[-2]
            for key in raw_features.keys():
                if key in prev and prev[key] != 0:
                    features[f"{key}_roc"] = (raw_features[key] - prev[key]) / prev[key]

        # Interactions
        features["spread_x_vol"] = (
            features.get("spread_bps_current", 0) *
            features.get("natr_14_5m_current", 0)
        )
        features["ofi_x_depth"] = (
            features.get("ofi_zscore_current", 0) *
            features.get("depth_imbalance_current", 0)
        )

        return features
```

---

## 12.9 Model Training Pipeline

```python
class MLTrainingPipeline:
    """End-to-end ML training pipeline."""

    def __init__(self, config: MLConfig):
        self.config = config

    def train(self, data_path: Path) -> TrainedModel:
        """Full training pipeline."""

        # 1. Load data
        data = self._load_data(data_path)

        # 2. Feature engineering
        features = self._engineer_features(data)

        # 3. Train/val/test split (temporal)
        train, val, test = self._temporal_split(features)

        # 4. Train model with hyperparameter tuning
        model = self._train_with_tuning(train, val)

        # 5. Evaluate on test set
        metrics = self._evaluate(model, test)

        # 6. SHAP analysis for interpretability
        shap_values = self._compute_shap(model, test)

        # 7. Generate report
        report = self._generate_report(model, metrics, shap_values)

        return TrainedModel(
            model=model,
            metrics=metrics,
            shap_values=shap_values,
            report=report,
            trained_at=datetime.now(),
            data_hash=self._hash_data(data),
        )

    def _train_with_tuning(self, train: pd.DataFrame,
                           val: pd.DataFrame) -> BaseEstimator:
        """Train with Optuna hyperparameter tuning."""

        def objective(trial):
            params = {
                "n_estimators": trial.suggest_int("n_estimators", 50, 200),
                "max_depth": trial.suggest_int("max_depth", 3, 8),
                "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
                "min_child_samples": trial.suggest_int("min_child_samples", 10, 100),
            }

            model = LGBMRegressor(**params)
            model.fit(
                train[self.feature_cols],
                train[self.target_col],
                eval_set=[(val[self.feature_cols], val[self.target_col])],
                callbacks=[early_stopping(50)],
            )

            preds = model.predict(val[self.feature_cols])
            return mean_squared_error(val[self.target_col], preds)

        study = optuna.create_study(direction="minimize")
        study.optimize(objective, n_trials=self.config.n_trials)

        # Train final model with best params
        best_model = LGBMRegressor(**study.best_params)
        best_model.fit(train[self.feature_cols], train[self.target_col])

        return best_model
```

---

## 12.10 Model Registry

```python
class ModelRegistry:
    """Registry for trained ML models."""

    def __init__(self, storage_path: Path):
        self.storage_path = storage_path
        self.metadata_file = storage_path / "registry.json"
        self.metadata: dict = self._load_metadata()

    def register(self, model: TrainedModel, name: str,
                 version: str) -> str:
        """Register a trained model."""
        model_id = f"{name}_{version}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # Save model
        model_path = self.storage_path / model_id
        model_path.mkdir(parents=True)
        joblib.dump(model.model, model_path / "model.joblib")

        # Save metadata
        self.metadata[model_id] = {
            "name": name,
            "version": version,
            "trained_at": model.trained_at.isoformat(),
            "metrics": model.metrics,
            "data_hash": model.data_hash,
            "status": "staged",  # staged -> production -> deprecated
        }
        self._save_metadata()

        return model_id

    def load(self, model_id: str) -> BaseEstimator:
        """Load a registered model."""
        model_path = self.storage_path / model_id / "model.joblib"
        return joblib.load(model_path)

    def promote_to_production(self, model_id: str) -> None:
        """Promote model to production."""
        # Demote current production model
        for mid, meta in self.metadata.items():
            if meta.get("status") == "production":
                meta["status"] = "deprecated"

        self.metadata[model_id]["status"] = "production"
        self._save_metadata()

    def get_production_model(self, name: str) -> BaseEstimator | None:
        """Get current production model."""
        for model_id, meta in self.metadata.items():
            if meta["name"] == name and meta["status"] == "production":
                return self.load(model_id)
        return None
```

---

## 12.11 Model Monitoring

```python
class ModelMonitor:
    """Monitor model performance in production."""

    def __init__(self, model_id: str):
        self.model_id = model_id
        self.predictions: list[tuple[dict, float, float]] = []  # (features, pred, actual)

    def record_prediction(self, features: dict,
                          prediction: float,
                          actual: float | None = None) -> None:
        """Record a prediction for monitoring."""
        self.predictions.append((features, prediction, actual))

    def check_drift(self) -> DriftReport:
        """Check for feature drift and prediction drift."""
        if len(self.predictions) < 100:
            return DriftReport(has_drift=False, details={})

        # Feature drift (compare recent vs historical)
        recent_features = [p[0] for p in self.predictions[-100:]]
        historical_features = [p[0] for p in self.predictions[:-100]]

        drift_scores = {}
        for feature in recent_features[0].keys():
            recent_vals = [f[feature] for f in recent_features]
            hist_vals = [f[feature] for f in historical_features]

            # KS test for drift
            ks_stat, p_value = ks_2samp(recent_vals, hist_vals)
            if p_value < 0.05:
                drift_scores[feature] = ks_stat

        # Prediction accuracy drift
        recent_with_actual = [p for p in self.predictions[-100:] if p[2] is not None]
        if recent_with_actual:
            recent_errors = [abs(p[1] - p[2]) for p in recent_with_actual]
            hist_errors = [abs(p[1] - p[2]) for p in self.predictions[:-100] if p[2] is not None]
            if hist_errors:
                accuracy_drift = np.mean(recent_errors) / np.mean(hist_errors) - 1

        return DriftReport(
            has_drift=len(drift_scores) > 0 or accuracy_drift > 0.2,
            feature_drift=drift_scores,
            accuracy_drift=accuracy_drift,
        )
```

---

## 12.12 Calibration Path

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CALIBRATION PATH                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  1. OFFLINE BACKTEST                                            â”‚
â”‚     â””â”€â”€ Walk-forward optimization                               â”‚
â”‚     â””â”€â”€ Parameter sweep                                         â”‚
â”‚     â””â”€â”€ Generate calibration dataset                            â”‚
â”‚                                                                  â”‚
â”‚  2. SHADOW MODE                                                 â”‚
â”‚     â””â”€â”€ Run ML predictions alongside rule-based                 â”‚
â”‚     â””â”€â”€ Compare performance (no real trades)                    â”‚
â”‚     â””â”€â”€ Collect prediction accuracy data                        â”‚
â”‚                                                                  â”‚
â”‚  3. CANARY DEPLOYMENT                                           â”‚
â”‚     â””â”€â”€ Small allocation (1-5% of capital)                      â”‚
â”‚     â””â”€â”€ Monitor real performance vs shadow                      â”‚
â”‚     â””â”€â”€ A/B test old vs new parameters                          â”‚
â”‚                                                                  â”‚
â”‚  4. PRODUCTION ROLLOUT                                          â”‚
â”‚     â””â”€â”€ Gradual increase in allocation                          â”‚
â”‚     â””â”€â”€ Continuous monitoring                                   â”‚
â”‚     â””â”€â”€ Automated rollback on degradation                       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 12.13 Model Configuration

```yaml
# config/ml.yaml
ml:
  enabled: true

  calibration:
    model: "lightgbm"
    retrain_interval_days: 7
    min_training_samples: 10000

  toxicity_prediction:
    model: "lightgbm"
    lookback_windows: [10, 30, 60]
    prediction_horizon_s: 60

  fill_probability:
    model: "lightgbm"
    features:
      - "distance_bps"
      - "spread_bps"
      - "ofi_zscore"
      - "depth_imbalance"
      - "natr_14_5m"

  monitoring:
    drift_check_interval_hours: 1
    accuracy_threshold: 0.7
    drift_threshold: 0.2

  registry:
    path: "models/"
    max_versions: 10
```
